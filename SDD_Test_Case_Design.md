# 3. Test Case Design

## 3.1 Unit Test Design
**PART A – Test Scope**
Unit tests for the First Increment focus on verifying the correctness of isolated logic within the backend services. The scope includes:
*   Validation logic for the "Business Description" submission (length, content type).
*   Parsing and normalization routines within the `SDFService` that convert raw AI JSON responses into the standardized System Definition File structure.
*   State management logic in the `ProjectService` (transitioning projects from 'Draft' to 'Analyzing' to 'Ready').
*   Authentication logic in the `AuthService` to ensure secure access to project endpoints.
*   Prompt construction functions in the AI Gateway that format user inputs for the LLM.

**PART B – Test Design Approach**
Unit tests are designed using a white-box methodology, inspecting the internal structure of code modules. Mock objects are used to simulate external dependencies, specifically the PostgreSQL database and the Google Gemini AI API. This ensures that tests run quickly and deterministically without network latency. This test level is necessary to catch logic errors, boundary conditions, and data type mismatches early in the development cycle, particularly given the complex data transformations required for the SDF.

**PART C – Test Case Format and Fields**
Unit test cases shall be formatted according to the following standard schema:
*   **Test Case ID:** Unique identifier for the test case (e.g., UT-001).
*   **Related Requirement ID:** Reference to the specific requirement or function being tested.
*   **Test Objective:** A concise statement of what logic is being verified.
*   **Preconditions:** State of the class or function inputs required before execution.
*   **Test Data / Mock Setup:** Specific inputs or mock configurations used.
*   **Test Steps:** Detailed sequence of actions to execute the unit of code.
*   **Expected Result:** The anticipated return value or state change.
*   **Actual Result:** Recorded outcome after execution.
*   **Pass/Fail Status:** Result of the comparison between Expected and Actual results.

## 3.2 Software Integration Test Design
**PART A – Test Scope**
Integration tests for the First Increment verify the successful interaction between the distinct architectural components. The scope includes:
*   The asynchronous workflow where the Node.js Backend submits a task to the Python AI Gateway.
*   The data flow of the `analyze` and `clarify` endpoints, ensuring the AI Gateway correctly receives context and returns a valid SDF.
*   The persistence layer, verifying that generated SDF objects and Project records are correctly stored and retrieved from the PostgreSQL database.
*   The interface between the Backend and the Assembler module responsible for packaging the generated Inventory module into a ZIP file.

**PART B – Test Design Approach**
Integration tests use a gray-box approach, treating individual services as black boxes while validating the data contracts (JSON payloads) passed between them. The approach verifies that the system components function correctly as a group within the Docker container environment. This test type is strictly necessary to detect interface defects, such as serialization errors between Node.js and Python, or connectivity issues with the database, which cannot be identified by unit testing.

**PART C – Test Case Format and Fields**
Integration test cases shall be formatted according to the following standard schema:
*   **Test Case ID:** Unique identifier for the test case (e.g., INT-001).
*   **Interface / API Endpoint ID:** Reference to the specific interface or API endpoint being tested.
*   **Test Objective:** Description of the interaction being verified.
*   **Preconditions:** Required state of the participating services (e.g., AI Gateway is healthy).
*   **Input Payload:** Data sent across the interface.
*   **Test Steps:** Sequence of operations to trigger the interaction.
*   **Expected Response / Side Effects:** Expected status codes, JSON responses, or database updates.
*   **Actual Result:** Observed response or system behavior.
*   **Pass/Fail Status:** Result of the verification.

## 3.3 System Test Design
**PART A – Test Scope**
System tests validate the end-to-end functionality of the First Increment as a complete product. The scope encompasses the full user journey:
*   Submitting a natural language business description via the Frontend Wizard.
*   Receiving and answering clarification questions generated by the system.
*   Reviewing the visual representation of the generated Inventory data model (SDF).
*   Triggering the generation process and successfully downloading the exported source code.
*   Verifying that out-of-scope features (e.g., Invoicing) are not accessible or present.

**PART B – Test Design Approach**
System tests are designed using a black-box methodology, interacting strictly with the Frontend UI (Project Wizard and Dashboard). The approach validates that the system meets the functional requirements defined in the Master Reference. It verifies error handling, loading states during asynchronous AI processing, and the usability of the clarification flow. This level is necessary to ensure the integrated increment delivers a cohesive and stable user experience under production-like conditions.

**PART C – Test Case Format and Fields**
System test cases shall be formatted according to the following standard schema:
*   **Test Case ID:** Unique identifier for the test case (e.g., SYS-001).
*   **Related Use Case (Increment 1):** Reference to the Use Case being exercised.
*   **Test Objective:** The specific functionality or scenario being validated.
*   **Preconditions:** User login state, database population, or configuration.
*   **Test Steps:** Step-by-step user actions on the interface.
*   **Expected Result:** Visible system responses, UI updates, or artifact downloads.
*   **Actual Result:** Observed system behavior.
*   **Pass/Fail Status:** Result of the validation.

## 3.4 Acceptance Test Design
**PART A – Test Scope**
Acceptance tests focus on validating the business value of the First Increment. The scope is limited to the correctness and utility of the generated "Inventory" module. It verifies that:
*   The generated entities (e.g., Product, Stock) match the semantic intent of the user's description.
*   The exported artifacts (Database Schema, API) are syntactically correct and deployable.
*   The system correctly handled specific business constraints (e.g., tracking expiration dates for food items).

**PART B – Test Design Approach**
Acceptance tests use a validation methodology driven by real-world business scenarios. These tests are designed to be performed by the Product Owner or Domain Expert. The approach involves defining high-level business goals (e.g., "Create a generic inventory system for a bookstore") and verifying that the output allows a user to perform those functions. This is the final gate to confirm the increment satisfies the "Inventory module generation" requirement.

**PART C – Test Case Format and Fields**
Acceptance test cases shall be formatted according to the following standard schema:
*   **Test Case ID:** Unique identifier for the test case (e.g., UAT-001).
*   **User Story / Business Goal:** Reference to the specific business need.
*   **Test Objective:** Verification of the semantic correctness of the output.
*   **Preconditions:** Operational environment setup.
*   **Test Steps:** Business-level actions performed by the user.
*   **Acceptance Criteria:** Specific conditions that must be met for approval.
*   **Expected Result:** The business outcome achieved (e.g., "Schema contains 'ISBN' field").
*   **Actual Result:** The actual outcome observed by the user.
*   **Pass/Fail Status:** Decision on whether to accept or reject the feature.
